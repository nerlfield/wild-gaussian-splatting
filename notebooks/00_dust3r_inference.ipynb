{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff14bf4-cec5-4ddc-8104-5409d475259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../dust3r')\n",
    "sys.path.append('../gaussian-splatting')\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe15815-ee0a-4e75-a1ed-d58e10e24758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust3r.inference import inference, load_model\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.utils.device import to_numpy\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import lovely_tensors as lt\n",
    "except:\n",
    "    ! pip install --upgrade lovely-tensors\n",
    "    import lovely_tensors as lt\n",
    "    \n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93933eb0-8885-4511-a8de-1071bdf2a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../dust3r/checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "device = 'cuda:0'\n",
    "batch_size = 1\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300\n",
    "scenegraph_type = \"complete\"\n",
    "winsize = 1\n",
    "refid = 0\n",
    "norm_scene = False\n",
    "mask_images = False # mask images by confidence threshold for 3DGS\n",
    "\n",
    "if scenegraph_type == \"swin\":\n",
    "    scenegraph_type = scenegraph_type + \"-\" + str(winsize)\n",
    "elif scenegraph_type == \"oneref\":\n",
    "    scenegraph_type = scenegraph_type + \"-\" + str(refid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b5195-0a67-401a-8817-a0e5b37e11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path.ls = lambda x: list(x.iterdir())\n",
    "\n",
    "image_dir = Path('../data/images/turtle_imgs/')\n",
    "\n",
    "image_files = [str(x) for x in image_dir.ls() if x.suffix in ['.png', '.jpg']]\n",
    "image_files = sorted(image_files, key=lambda x: int(x.split('/')[-1].split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8d0fd-3aa4-41f2-ba88-3f257173d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_path, device)\n",
    "images = load_images(image_files, size=512)\n",
    "pairs = make_pairs(images, scene_graph='complete', prefilter=None, symmetrize=True)\n",
    "output = inference(pairs, model, device, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595b9e7-3eb0-4a44-9505-bb97b3bb488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PointCloudOptimizer)\n",
    "loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb023ce6-ad41-40e4-9d91-66afa4222412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colmap_dataset_utils import inv\n",
    "\n",
    "intrinsics = scene.get_intrinsics().detach().cpu().numpy()\n",
    "cam2world = scene.get_im_poses().detach().cpu().numpy()\n",
    "world2cam = inv(cam2world) #\n",
    "principal_points = scene.get_principal_points().detach().cpu().numpy()\n",
    "focals = scene.get_focals().detach().cpu().numpy()\n",
    "imgs = np.array(scene.imgs)\n",
    "pts3d = [i.detach() for i in scene.get_pts3d()]\n",
    "depth_maps = [i.detach() for i in scene.get_depthmaps()]\n",
    "\n",
    "min_conf_thr = 20\n",
    "scene.min_conf_thr = float(scene.conf_trf(torch.tensor(min_conf_thr)))\n",
    "masks = to_numpy(scene.get_masks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08673e-d4af-414e-9d57-9b053f7be81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colmap_dataset_utils import normalize_scene\n",
    "from copy import deepcopy\n",
    "\n",
    "pts_norm, c2w_norm = normalize_scene(deepcopy(pts3d), deepcopy(masks), deepcopy(cam2world))\n",
    "if norm_scene:\n",
    "    pts3d = pts_norm\n",
    "    world2cam = inv(np.array(c2w_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3877e-baf5-4944-8819-c968b9e7550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import visualize_pcd, visualize_cameras\n",
    "fig = None\n",
    "\n",
    "num_to_show = 10_000\n",
    "num_of_valid = sum([m.sum() for m in masks])\n",
    "skip = num_of_valid // num_to_show\n",
    "\n",
    "for p, i, m, c2w in zip(pts_norm, imgs, masks, c2w_norm):\n",
    "    fig = visualize_pcd(p[m].cpu().numpy(), i[m], skip=skip, show=False, size=2, fig=fig)\n",
    "    R, T = np.transpose(c2w[None, :3, :3], (0, 2, 1)), c2w[None, :, 3]\n",
    "    fig = visualize_cameras(R, T, fig=fig, show=False, radius=2, size=0.2)\n",
    "    \n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6184d-151c-4b6d-9910-6d497f57d44c",
   "metadata": {},
   "source": [
    "# Construct colmap dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d82908-d722-4bd8-be6e-55a88798481e",
   "metadata": {},
   "source": [
    "After convertion such data sctructure should appear\n",
    "\n",
    "```\n",
    "│   │   │   ├── images\n",
    "│   │   │   ├── masks\n",
    "│   │   │   ├── sparse/0\n",
    "|   |   |   |    |------cameras.bin\n",
    "|   |   |   |    |------images.bin\n",
    "|   |   |   |    |------points3D.bin\n",
    "|   |   |   |    |------points3D.ply\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54baea-2a00-4872-a64b-be90872907b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path('../data/scenes/turtle')\n",
    "save_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888ccf6-c8f7-4a15-a129-61be14ce3a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colmap_dataset_utils import (\n",
    "    init_filestructure,\n",
    "    save_images_masks,\n",
    "    save_cameras,\n",
    "    save_imagestxt,\n",
    "    save_pointcloud_with_normals\n",
    ")\n",
    "\n",
    "save_path, images_path, masks_path, sparse_path = init_filestructure(save_dir)\n",
    "save_images_masks(imgs, masks, images_path, masks_path, mask_images)\n",
    "save_cameras(focals, principal_points, sparse_path, imgs_shape=imgs.shape)\n",
    "save_imagestxt(world2cam, sparse_path)\n",
    "# save_pointcloud(imgs, pts3d, masks, sparse_path)\n",
    "save_pointcloud_with_normals(imgs, pts3d, masks, sparse_path)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

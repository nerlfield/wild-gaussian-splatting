{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16411955-62d7-42fb-a4b2-190b8e960abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DUST3R_DATASET = True\n",
    "\n",
    "dataset_path = \"../data/scenes/turtle/\" if USE_DUST3R_DATASET else \"../data/scenes/turtle_mast3r/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c5023-7bce-447f-bac2-f154102be7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../gaussian-splatting')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from random import randint\n",
    "from utils.loss_utils import l1_loss, ssim\n",
    "from gaussian_renderer import render, network_gui\n",
    "from scene import Scene, GaussianModel\n",
    "from utils.general_utils import safe_state\n",
    "import uuid\n",
    "from tqdm.auto import tqdm\n",
    "from utils.image_utils import psnr\n",
    "from train import prepare_output_and_logger, training_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc390e-27d7-434f-9ddc-a364c52533bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import lovely_tensors as lt\n",
    "except:\n",
    "    ! pip install --upgrade lovely-tensors\n",
    "    import lovely_tensors as lt\n",
    "    \n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624ff7f-36e3-49fd-8794-ed07ca7515d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class ModelParams:\n",
    "    sh_degree: int = 3\n",
    "    source_path: str = dataset_path\n",
    "    model_path: str = \"\"\n",
    "    images: str = \"images\"\n",
    "    resolution: int = -1\n",
    "    white_background: bool = True\n",
    "    data_device: str = \"cuda\"\n",
    "    eval: bool = False\n",
    "\n",
    "    def post_init(self):\n",
    "        self.source_path = os.path.abspath(self.source_path)\n",
    "\n",
    "@dataclass\n",
    "class PipelineParams:\n",
    "    convert_SHs_python: bool = False\n",
    "    compute_cov3D_python: bool = False\n",
    "    debug: bool = False\n",
    "\n",
    "@dataclass\n",
    "class OptimizationParams:\n",
    "    iterations = 7001 #30_000\n",
    "    position_lr_init = 0.00016\n",
    "    position_lr_final = 0.0000016\n",
    "    position_lr_delay_mult = 0.01\n",
    "    position_lr_max_steps = 30_000\n",
    "    feature_lr = 0.0025\n",
    "    opacity_lr = 0.05\n",
    "    scaling_lr = 0.005\n",
    "    rotation_lr = 0.001\n",
    "    percent_dense = 0.01\n",
    "    lambda_dssim = 0.2\n",
    "    densification_interval = 100\n",
    "    opacity_reset_interval = 3000\n",
    "    densify_from_iter = 500\n",
    "    densify_until_iter = 15_000\n",
    "    densify_grad_threshold = 0.0002\n",
    "    random_background = False\n",
    "\n",
    "@dataclass\n",
    "class TrainingArgs:\n",
    "    ip: str = \"0.0.0.0\"\n",
    "    port: int = 6007\n",
    "    debug_from: int = -1\n",
    "    detect_anomaly: bool = False\n",
    "    test_iterations: list[int] = field(default_factory=lambda: [7_000, 30_000])\n",
    "    save_iterations: list[int] = field(default_factory=lambda: [7_000, 30_000])\n",
    "    quiet: bool = False\n",
    "    checkpoint_iterations: list[int] = field(default_factory=lambda: [7_000, 15_000, 30_000])\n",
    "    start_checkpoint: str = None\n",
    "    \n",
    "dataset = ModelParams()\n",
    "opt = OptimizationParams()\n",
    "pipe = PipelineParams()\n",
    "args = TrainingArgs()\n",
    "\n",
    "testing_iterations = args.test_iterations\n",
    "saving_iterations = args.save_iterations \n",
    "checkpoint_iterations = args.checkpoint_iterations \n",
    "checkpoint = args.start_checkpoint\n",
    "debug_from = args.debug_from\n",
    "\n",
    "tb_writer = prepare_output_and_logger(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e20d91-c2f3-46c1-9f55-d5577607e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussians = GaussianModel(dataset.sh_degree)\n",
    "scene = Scene(dataset, gaussians)\n",
    "gaussians.training_setup(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e82742-6462-4243-bc13-47e522cfad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_color = [1, 1, 1] if dataset.white_background else [0, 0, 0]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d588696-bb42-4f41-98e6-f50a8ab8dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_start = torch.cuda.Event(enable_timing = True)\n",
    "iter_end = torch.cuda.Event(enable_timing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a8e79-a80b-4551-b0ca-7b850a81e00c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viewpoint_stack = None\n",
    "ema_loss_for_log = 0.0\n",
    "first_iter = 0\n",
    "progress_bar = tqdm(range(first_iter, opt.iterations), desc=\"Training progress\")\n",
    "first_iter += 1\n",
    "for iteration in range(first_iter, opt.iterations + 1):\n",
    "    iter_start.record()\n",
    "    gaussians.update_learning_rate(iteration)\n",
    "    \n",
    "    # Every 1000 its we increase the levels of SH up to a maximum degree\n",
    "    if iteration % 1000 == 0:\n",
    "        gaussians.oneupSHdegree()\n",
    "        \n",
    "    # Pick a random Camera\n",
    "    if not viewpoint_stack:\n",
    "        viewpoint_stack = scene.getTrainCameras().copy()\n",
    "    viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack)-1))\n",
    "    \n",
    "    # Render\n",
    "    if (iteration - 1) == debug_from:\n",
    "        pipe.debug = True\n",
    "    bg = torch.rand((3), device=\"cuda\") if opt.random_background else background\n",
    "    \n",
    "    render_pkg = render(viewpoint_cam, gaussians, pipe, bg)\n",
    "    image, viewspace_point_tensor, visibility_filter, radii = render_pkg[\"render\"], render_pkg[\"viewspace_points\"], render_pkg[\"visibility_filter\"], render_pkg[\"radii\"]\n",
    "\n",
    "    # Loss\n",
    "    gt_image = viewpoint_cam.original_image.cuda()\n",
    "    Ll1 = l1_loss(image, gt_image)\n",
    "    loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image))\n",
    "    loss.backward()\n",
    "    iter_end.record()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Progress bar\n",
    "        ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log\n",
    "        if iteration % 10 == 0:\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{ema_loss_for_log:.{7}f}\"})\n",
    "            progress_bar.update(10)\n",
    "        if iteration == opt.iterations:\n",
    "            progress_bar.close()\n",
    "\n",
    "        # Log and save\n",
    "        training_report(tb_writer, iteration, Ll1, loss, l1_loss, iter_start.elapsed_time(iter_end), testing_iterations, scene, render, (pipe, background))\n",
    "        if (iteration in saving_iterations):\n",
    "            print(\"\\n[ITER {}] Saving Gaussians\".format(iteration))\n",
    "            scene.save(iteration)\n",
    "\n",
    "        # Densification\n",
    "        if iteration < opt.densify_until_iter:\n",
    "            # Keep track of max radii in image-space for pruning\n",
    "            gaussians.max_radii2D[visibility_filter] = torch.max(gaussians.max_radii2D[visibility_filter], radii[visibility_filter])\n",
    "            gaussians.add_densification_stats(viewspace_point_tensor, visibility_filter)\n",
    "\n",
    "            if iteration > opt.densify_from_iter and iteration % opt.densification_interval == 0:\n",
    "                size_threshold = 20 if iteration > opt.opacity_reset_interval else None\n",
    "                gaussians.densify_and_prune(opt.densify_grad_threshold, 0.005, scene.cameras_extent, size_threshold)\n",
    "\n",
    "            if iteration % opt.opacity_reset_interval == 0 or (dataset.white_background and iteration == opt.densify_from_iter):\n",
    "                gaussians.reset_opacity()\n",
    "\n",
    "        # Optimizer step\n",
    "        if iteration < opt.iterations:\n",
    "            gaussians.optimizer.step()\n",
    "            gaussians.optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "        if (iteration in checkpoint_iterations):\n",
    "            print(\"\\n[ITER {}] Saving Checkpoint\".format(iteration))\n",
    "            torch.save((gaussians.capture(), iteration), scene.model_path + \"/chkpnt\" + str(iteration) + \".pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be6590f-01dc-4d4e-9bd1-bfa6dcd6b88b",
   "metadata": {},
   "source": [
    "# Render scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a3b4d-cc31-423e-b364-2530af86acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from utils.graphics_utils import focal2fov, fov2focal, getProjectionMatrix\n",
    "import torchvision\n",
    "import subprocess\n",
    "\n",
    "@torch.no_grad()\n",
    "def render_path(dataset : ModelParams, iteration : int, pipeline : PipelineParams, render_resize_method='crop'):\n",
    "    \"\"\"\n",
    "    render_resize_method: crop, pad\n",
    "    \"\"\"\n",
    "    gaussians = GaussianModel(dataset.sh_degree)\n",
    "    scene = Scene(dataset, gaussians, load_iteration=iteration, shuffle=False)\n",
    "\n",
    "    iteration = scene.loaded_iter\n",
    "\n",
    "    bg_color = [1,1,1] if dataset.white_background else [0, 0, 0]\n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    model_path = dataset.model_path\n",
    "    name = \"render\"\n",
    "\n",
    "    views = scene.getRenderCameras()\n",
    "\n",
    "    # print(len(views))\n",
    "    render_path = os.path.join(model_path, name, \"ours_{}\".format(iteration), \"renders\")\n",
    "\n",
    "    makedirs(render_path, exist_ok=True)\n",
    "\n",
    "    for idx, view in enumerate(tqdm(views, desc=\"Rendering progress\")):\n",
    "        if render_resize_method == 'crop':\n",
    "            image_size = 512\n",
    "        elif render_resize_method == 'pad':\n",
    "            image_size = max(view.image_width, view.image_height)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        view.original_image = torch.zeros((3, image_size, image_size), device=view.original_image.device)\n",
    "        focal_length_x = fov2focal(view.FoVx, view.image_width)\n",
    "        focal_length_y = fov2focal(view.FoVy, view.image_height)\n",
    "        view.image_width = image_size\n",
    "        view.image_height = image_size\n",
    "        view.FoVx = focal2fov(focal_length_x, image_size)\n",
    "        view.FoVy = focal2fov(focal_length_y, image_size)\n",
    "        view.projection_matrix = getProjectionMatrix(znear=view.znear, zfar=view.zfar, fovX=view.FoVx, fovY=view.FoVy).transpose(0,1).cuda().float()\n",
    "        view.full_proj_transform = (view.world_view_transform.unsqueeze(0).bmm(view.projection_matrix.unsqueeze(0))).squeeze(0)\n",
    "\n",
    "        render_pkg = render(view, gaussians, pipeline, background)\n",
    "        rendering = render_pkg[\"render\"]\n",
    "        torchvision.utils.save_image(rendering, os.path.join(render_path, '{0:05d}'.format(idx) + \".png\"))\n",
    "\n",
    "    # Use ffmpeg to output video\n",
    "    renders_path = os.path.join(model_path, name, \"ours_{}\".format(iteration), \"renders.mp4\")\n",
    "    # Use ffmpeg to output video\n",
    "    subprocess.run([\"ffmpeg\", \"-y\", \n",
    "                \"-framerate\", \"24\",\n",
    "                \"-i\", os.path.join(render_path, \"%05d.png\"), \n",
    "                \"-vf\", \"pad=ceil(iw/2)*2:ceil(ih/2)*2\",\n",
    "                \"-c:v\", \"libx264\", \n",
    "                \"-pix_fmt\", \"yuv420p\",\n",
    "                \"-crf\", \"23\", \n",
    "                # \"-pix_fmt\", \"yuv420p\",  # Set pixel format for compatibility\n",
    "                renders_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafcc42d-a165-48fc-bfa8-80e916275181",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_path(dataset, 7000, pipe, render_resize_method='crop')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
